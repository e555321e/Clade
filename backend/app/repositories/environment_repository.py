from __future__ import annotations

import logging
import time
from collections.abc import Iterable
from typing import Generator

from pathlib import Path

from sqlalchemy import text, Index
from sqlalchemy.exc import OperationalError

logger = logging.getLogger(__name__)
from sqlalchemy.sql import func
from sqlmodel import select

from ..core.database import session_scope
from ..models.environment import (
    EnvironmentEvent,
    HabitatPopulation,
    MapState,
    MapTile,
)
from ..models.config import UIConfig, ProviderConfig


class EnvironmentRepository:
    """环境数据仓储
    
    【性能优化】v2.0
    1. 批量插入操作（write_habitats_bulk）- 10x 速度提升
    2. 只获取最新回合数据（list_latest_habitats）- 70% 数据减少
    3. 历史数据清理（cleanup_old_habitats）- 控制数据膨胀
    4. 数据库索引优化（ensure_indexes）- 查询加速
    5. 分块迭代器（iter_habitats_chunked）- 降低内存峰值
    """
    def upsert_tiles(self, tiles: Iterable[MapTile]) -> None:
        with session_scope() as session:
            for tile in tiles:
                session.merge(tile)

    def list_tiles(self, limit: int | None = None) -> list[MapTile]:
        with session_scope() as session:
            stmt = select(MapTile)
            if limit:
                stmt = stmt.limit(limit)
            return list(session.exec(stmt))

    def log_event(self, event: EnvironmentEvent) -> EnvironmentEvent:
        with session_scope() as session:
            session.add(event)
            session.flush()
            session.refresh(event)
            return event

    def get_state(self) -> MapState | None:
        with session_scope() as session:
            return session.exec(select(MapState)).first()

    def save_state(self, state: MapState) -> MapState:
        with session_scope() as session:
            merged = session.merge(state)
            session.flush()
            session.refresh(merged)
            return merged

    def clear_state(self) -> None:
        """清除所有环境相关数据（用于读档前）"""
        with session_scope() as session:
            # 先删除依赖表
            session.exec(text("DELETE FROM habitat_populations"))
            session.exec(text("DELETE FROM environment_events"))
            # 再删除主表
            session.exec(text("DELETE FROM map_tiles"))
            session.exec(text("DELETE FROM map_state"))

    def ensure_tile_columns(self) -> None:
        with session_scope() as session:
            info = session.exec(text("PRAGMA table_info('map_tiles')")).all()
            if not info:
                return
            columns = {row[1] for row in info}
            if "q" not in columns:
                session.exec(text("ALTER TABLE map_tiles ADD COLUMN q INTEGER DEFAULT 0"))
            if "r" not in columns:
                session.exec(text("ALTER TABLE map_tiles ADD COLUMN r INTEGER DEFAULT 0"))
            if "salinity" not in columns:
                print("[环境仓储] 添加 salinity 列...")
                session.exec(text("ALTER TABLE map_tiles ADD COLUMN salinity REAL DEFAULT 35.0"))
            if "is_lake" not in columns:
                print("[环境仓储] 添加 is_lake 列...")
                session.exec(text("ALTER TABLE map_tiles ADD COLUMN is_lake BOOLEAN DEFAULT 0"))
    
    def ensure_map_state_columns(self) -> None:
        """确保 map_state 表包含海平面和温度字段"""
        with session_scope() as session:
            info = session.exec(text("PRAGMA table_info('map_state')")).all()
            if not info:
                return
            columns = {row[1] for row in info}
            if "sea_level" not in columns:
                print("[环境仓储] 添加 sea_level 列...")
                session.exec(text("ALTER TABLE map_state ADD COLUMN sea_level REAL DEFAULT 0.0"))
            if "global_avg_temperature" not in columns:
                print("[环境仓储] 添加 global_avg_temperature 列...")
                session.exec(text("ALTER TABLE map_state ADD COLUMN global_avg_temperature REAL DEFAULT 15.0"))
            if "map_seed" not in columns:
                print("[环境仓储] 添加 map_seed 列...")
                session.exec(text("ALTER TABLE map_state ADD COLUMN map_seed INTEGER DEFAULT NULL"))

    def load_ui_config(self, path: Path) -> UIConfig:
        """仅从 JSON 文件加载 UI 配置"""
        # 1. 从 JSON 文件加载
        if path.exists():
            config = UIConfig.model_validate_json(path.read_text(encoding="utf-8"))
        else:
            config = UIConfig()
        return config

    def save_ui_config(self, path: Path, config: UIConfig) -> UIConfig:
        """保存 UI 配置到 JSON 文件"""
        # 1. 保存到 JSON 文件
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(config.model_dump_json(indent=2, ensure_ascii=False), encoding="utf-8")
        logger.debug(f"[配置] 已保存配置到 {path}")
        return config

    def list_habitats(self) -> list[HabitatPopulation]:
        with session_scope() as session:
            return list(session.exec(select(HabitatPopulation)))

    def write_habitats(self, habitats: Iterable[HabitatPopulation]) -> None:
        with session_scope() as session:
            for habitat in habitats:
                # 容错：确保类型合法，避免 sqlite "type 'set' is not supported"
                try:
                    if not isinstance(habitat.population, int):
                        habitat.population = int(habitat.population or 0)
                except Exception:
                    habitat.population = 0
                
                ti = getattr(habitat, "turn_index", 0)
                needs_fix = False
                original_type = type(ti).__name__
                original_value = ti
                
                if not isinstance(ti, int):
                    # 如果传入了 set/list 等异常类型，回退为最大值或 0
                    needs_fix = True
                    try:
                        if isinstance(ti, (set, list, tuple)):
                            ti = max(ti) if ti else 0
                        else:
                            ti = int(ti)
                    except Exception:
                        ti = 0
                
                # 【新增】检测异常大的 turn_index（可能是字段赋值错误）
                # 正常游戏不太可能超过 10000 回合
                if ti > 10000:
                    needs_fix = True
                    logger.error(
                        f"[环境仓储] 检测到异常大的 turn_index={ti}，"
                        f"可能是 species_id={habitat.species_id} 或 tile_id={habitat.tile_id} 被错误赋值！"
                        f"请检查存档数据或上游代码。"
                    )
                    # 不自动修正，但记录警告以便追踪
                
                if needs_fix and ti <= 10000:
                    habitat.turn_index = ti
                    logger.warning(
                        f"[环境仓储] 修正非法 turn_index: "
                        f"species_id={habitat.species_id}, tile_id={habitat.tile_id}, "
                        f"原类型={original_type}, 原值={repr(original_value)[:100]}, 修正为={ti}"
                    )
                
                try:
                    session.add(habitat)
                except Exception as e:
                    logger.error(f"[环境仓储] 写入栖息地失败: {e} | habitat={habitat}")
                    raise

    def latest_habitats(
        self,
        species_ids: list[int] | None = None,
        limit: int | None = None,
        per_species_latest: bool = True,
    ) -> list[HabitatPopulation]:
        """获取最新的栖息地记录
        
        Args:
            species_ids: 可选，只获取指定物种的记录
            limit: 可选，限制返回数量
            per_species_latest: 【改进】如果为True，对每个物种取其最新turn的记录；
                               如果为False，取全局max_turn的记录（旧行为）
        
        Returns:
            list[HabitatPopulation]: 栖息地记录列表
        """
        with session_scope() as session:
            if not per_species_latest:
                # 旧逻辑：只取全局 max_turn
                max_turn = session.exec(select(func.max(HabitatPopulation.turn_index))).one()
                if max_turn is None:
                    return []
                stmt = (
                    select(HabitatPopulation)
                    .where(HabitatPopulation.turn_index == max_turn)
                    .order_by(HabitatPopulation.population.desc())
                )
                if species_ids:
                    stmt = stmt.where(HabitatPopulation.species_id.in_(species_ids))
                if limit:
                    stmt = stmt.limit(limit)
                return list(session.exec(stmt))
            
            # 【改进】新逻辑：对每个物种取其最新 turn_index 的记录
            # 这样即使某物种在当前回合没有写入新记录，也能取到它最近一次的栖息地数据
            
            # 子查询：获取每个物种的最新 turn_index
            from sqlmodel import col
            subquery = (
                select(
                    HabitatPopulation.species_id,
                    func.max(HabitatPopulation.turn_index).label("max_turn")
                )
                .group_by(HabitatPopulation.species_id)
            )
            if species_ids:
                subquery = subquery.where(HabitatPopulation.species_id.in_(species_ids))
            subquery = subquery.subquery()
            
            # 主查询：关联子查询获取每个物种最新回合的所有记录
            stmt = (
                select(HabitatPopulation)
                .join(
                    subquery,
                    (HabitatPopulation.species_id == subquery.c.species_id) &
                    (HabitatPopulation.turn_index == subquery.c.max_turn)
                )
                .order_by(HabitatPopulation.population.desc())
            )
            if limit:
                stmt = stmt.limit(limit)
            return list(session.exec(stmt))
    
    def get_species_with_habitats(self, current_turn_only: bool = False) -> set[int]:
        """获取所有有栖息地记录的物种ID集合
        
        Args:
            current_turn_only: 如果为True，只返回最新回合有记录的物种；
                              如果为False，返回所有有过栖息地记录的物种（默认）
        
        Returns:
            set[int]: 有栖息地记录的物种ID集合
        """
        with session_scope() as session:
            if current_turn_only:
                # 旧逻辑：只看全局 max_turn
                max_turn = session.exec(select(func.max(HabitatPopulation.turn_index))).one()
                if max_turn is None:
                    return set()
                stmt = select(HabitatPopulation.species_id).where(HabitatPopulation.turn_index == max_turn).distinct()
            else:
                # 【改进】返回所有有过栖息地记录的物种（不限制回合）
                stmt = select(HabitatPopulation.species_id).distinct()
            result = session.exec(stmt).all()
            return set(result)
    
    def get_habitats_by_species_id(self, species_id: int, latest_only: bool = True) -> list[HabitatPopulation]:
        """获取指定物种的栖息地记录（用于分化时继承栖息地）
        
        Args:
            species_id: 物种ID
            latest_only: 是否只获取该物种最新回合的记录（默认True）
                        【改进】现在取的是该物种自己的最新turn，而非全局max_turn
            
        Returns:
            list[HabitatPopulation]: 该物种的栖息地记录列表
        """
        with session_scope() as session:
            if latest_only:
                # 【改进】取该物种自己的最新 turn_index，而非全局 max_turn
                species_max_turn = session.exec(
                    select(func.max(HabitatPopulation.turn_index))
                    .where(HabitatPopulation.species_id == species_id)
                ).one()
                if species_max_turn is None:
                    return []
                stmt = (
                    select(HabitatPopulation)
                    .where(HabitatPopulation.species_id == species_id)
                    .where(HabitatPopulation.turn_index == species_max_turn)
                )
            else:
                stmt = select(HabitatPopulation).where(HabitatPopulation.species_id == species_id)
            return list(session.exec(stmt))

    def get_tile_coordinates_map(self) -> dict[int, tuple[int, int]]:
        """获取所有地块的坐标映射 {tile_id: (x, y)}
        
        用于批量处理物种迁移时的坐标查找
        """
        with session_scope() as session:
            # 只查询需要的列
            stmt = select(MapTile.id, MapTile.x, MapTile.y)
            results = session.exec(stmt).all()
            return {row[0]: (row[1], row[2]) for row in results if row[0] is not None}

    # ==================== 性能优化方法 ====================

    def list_latest_habitats(self) -> list[HabitatPopulation]:
        """只获取最新回合的栖息地数据（用于存档保存）
        
        【性能优化】相比 list_habitats() 减少 70%+ 数据量
        
        Returns:
            最新回合的所有栖息地记录
        """
        with session_scope() as session:
            max_turn = session.exec(
                select(func.max(HabitatPopulation.turn_index))
            ).one()
            if max_turn is None:
                return []
            
            stmt = select(HabitatPopulation).where(
                HabitatPopulation.turn_index == max_turn
            )
            return list(session.exec(stmt))

    def write_habitats_bulk(
        self, 
        habitats_data: list[dict],
        chunk_size: int = 5000
    ) -> int:
        """批量插入栖息地数据（高性能）
        
        【性能优化】使用 SQLAlchemy Core 批量插入，比逐条插入快 10x+
        
        Args:
            habitats_data: 栖息地数据字典列表
            chunk_size: 每批插入数量
            
        Returns:
            插入的记录数
        """
        if not habitats_data:
            return 0
        
        total_inserted = 0
        start_time = time.time()
        
        with session_scope() as session:
            # 分块插入，避免单次事务过大
            for i in range(0, len(habitats_data), chunk_size):
                chunk = habitats_data[i:i + chunk_size]
                
                # 数据清洗
                cleaned_chunk = []
                for h in chunk:
                    # 确保类型正确
                    cleaned = {
                        'tile_id': int(h.get('tile_id', 0)),
                        'species_id': int(h.get('species_id', 0)),
                        'population': int(h.get('population', 0)),
                        'suitability': float(h.get('suitability', 0.0)),
                        'turn_index': int(h.get('turn_index', 0)),
                    }
                    # 保留 id 如果存在
                    if h.get('id'):
                        cleaned['id'] = int(h['id'])
                    cleaned_chunk.append(cleaned)
                
                # 使用 Core API 批量插入
                session.execute(
                    HabitatPopulation.__table__.insert(),
                    cleaned_chunk
                )
                total_inserted += len(cleaned_chunk)
                
                # 每批次提交，避免长事务
                session.commit()
        
        elapsed = time.time() - start_time
        logger.info(
            f"[环境仓储] 批量插入 {total_inserted} 条栖息地记录，"
            f"耗时 {elapsed:.2f}s ({total_inserted/max(elapsed, 0.001):.0f} 条/秒)"
        )
        return total_inserted

    def iter_habitats_chunked(
        self, 
        chunk_size: int = 10000
    ) -> Generator[list[HabitatPopulation], None, None]:
        """分块迭代栖息地数据（低内存）
        
        【性能优化】用于大数据量场景，避免一次性加载全部数据
        
        Args:
            chunk_size: 每块大小
            
        Yields:
            栖息地记录块
        """
        with session_scope() as session:
            # 获取总数
            total = session.exec(
                select(func.count(HabitatPopulation.id))
            ).one() or 0
            
            if total == 0:
                return
            
            # 分块查询
            offset = 0
            while offset < total:
                stmt = (
                    select(HabitatPopulation)
                    .order_by(HabitatPopulation.id)
                    .offset(offset)
                    .limit(chunk_size)
                )
                chunk = list(session.exec(stmt))
                if not chunk:
                    break
                yield chunk
                offset += len(chunk)

    def cleanup_old_habitats(self, keep_turns: int = 3) -> int:
        """清理旧的栖息地历史数据
        
        【性能优化】只保留最近 N 回合的数据，控制数据库膨胀
        
        Args:
            keep_turns: 保留最近多少回合的数据
            
        Returns:
            删除的记录数
        """
        with session_scope() as session:
            max_turn = session.exec(
                select(func.max(HabitatPopulation.turn_index))
            ).one()
            
            if max_turn is None:
                return 0
            
            cutoff = max_turn - keep_turns
            if cutoff < 0:
                return 0
            
            # 删除旧数据
            result = session.execute(
                text(f"DELETE FROM habitat_populations WHERE turn_index < :cutoff"),
                {"cutoff": cutoff}
            )
            deleted = result.rowcount
            session.commit()
            
            if deleted > 0:
                logger.info(
                    f"[环境仓储] 清理旧栖息地数据: 删除 {deleted} 条 "
                    f"(保留 turn >= {cutoff})"
                )
            
            return deleted

    def get_habitat_stats(self) -> dict:
        """获取栖息地数据统计信息
        
        Returns:
            统计信息字典
        """
        with session_scope() as session:
            total = session.exec(
                select(func.count(HabitatPopulation.id))
            ).one() or 0
            
            min_turn = session.exec(
                select(func.min(HabitatPopulation.turn_index))
            ).one()
            
            max_turn = session.exec(
                select(func.max(HabitatPopulation.turn_index))
            ).one()
            
            species_count = session.exec(
                select(func.count(func.distinct(HabitatPopulation.species_id)))
            ).one() or 0
            
            # 计算每回合平均记录数
            turn_count = (max_turn - min_turn + 1) if max_turn and min_turn else 0
            avg_per_turn = total / turn_count if turn_count > 0 else 0
            
            return {
                "total_records": total,
                "min_turn": min_turn,
                "max_turn": max_turn,
                "turn_count": turn_count,
                "species_count": species_count,
                "avg_records_per_turn": round(avg_per_turn, 0),
                "estimated_size_mb": round(total * 50 / 1024 / 1024, 2),  # 估算每条 50 字节
            }

    def ensure_indexes(self) -> dict[str, bool]:
        """确保数据库索引存在（性能优化）
        
        【重要】在大数据量场景下，索引可以将查询速度提升 10-100x
        
        Returns:
            创建结果 {索引名: 是否新建}
        """
        results = {}
        
        index_definitions = [
            # 栖息地表索引
            ("idx_habitat_turn", "habitat_populations", "turn_index"),
            ("idx_habitat_species", "habitat_populations", "species_id"),
            ("idx_habitat_tile", "habitat_populations", "tile_id"),
            ("idx_habitat_species_turn", "habitat_populations", "species_id, turn_index"),
            ("idx_habitat_tile_turn", "habitat_populations", "tile_id, turn_index"),
            # 地块表索引
            ("idx_tile_xy", "map_tiles", "x, y"),
            ("idx_tile_qr", "map_tiles", "q, r"),
            ("idx_tile_biome", "map_tiles", "biome"),
            ("idx_tile_plate", "map_tiles", "plate_id"),
        ]
        
        with session_scope() as session:
            for idx_name, table, columns in index_definitions:
                try:
                    # 检查索引是否存在
                    check_sql = f"SELECT name FROM sqlite_master WHERE type='index' AND name='{idx_name}'"
                    existing = session.execute(text(check_sql)).fetchone()
                    
                    if existing:
                        results[idx_name] = False  # 已存在
                    else:
                        # 创建索引
                        create_sql = f"CREATE INDEX IF NOT EXISTS {idx_name} ON {table} ({columns})"
                        session.execute(text(create_sql))
                        session.commit()
                        results[idx_name] = True  # 新创建
                        logger.info(f"[环境仓储] 创建索引: {idx_name} ON {table}({columns})")
                except OperationalError as e:
                    logger.warning(f"[环境仓储] 创建索引 {idx_name} 失败: {e}")
                    results[idx_name] = False
        
        return results

    def optimize_database(self) -> dict[str, any]:
        """优化数据库（VACUUM + ANALYZE）
        
        【重要】定期调用可以：
        1. 回收已删除数据的空间
        2. 更新查询优化器统计信息
        
        Returns:
            优化结果信息
        """
        results = {
            "vacuum": False,
            "analyze": False,
            "indexes_created": {},
        }
        
        start_time = time.time()
        
        with session_scope() as session:
            try:
                # VACUUM 需要在事务外执行
                session.execute(text("VACUUM"))
                results["vacuum"] = True
                logger.info("[环境仓储] VACUUM 完成")
            except Exception as e:
                logger.warning(f"[环境仓储] VACUUM 失败: {e}")
            
            try:
                session.execute(text("ANALYZE"))
                results["analyze"] = True
                logger.info("[环境仓储] ANALYZE 完成")
            except Exception as e:
                logger.warning(f"[环境仓储] ANALYZE 失败: {e}")
        
        # 确保索引
        results["indexes_created"] = self.ensure_indexes()
        
        elapsed = time.time() - start_time
        results["elapsed_seconds"] = round(elapsed, 2)
        
        logger.info(f"[环境仓储] 数据库优化完成，耗时 {elapsed:.2f}s")
        return results

    def upsert_tiles_bulk(self, tiles_data: list[dict], chunk_size: int = 1000) -> int:
        """批量更新/插入地块数据
        
        Args:
            tiles_data: 地块数据字典列表
            chunk_size: 每批处理数量
            
        Returns:
            处理的记录数
        """
        if not tiles_data:
            return 0
        
        total = 0
        start_time = time.time()
        
        with session_scope() as session:
            for i in range(0, len(tiles_data), chunk_size):
                chunk = tiles_data[i:i + chunk_size]
                
                for tile_data in chunk:
                    # 使用 merge 实现 upsert
                    tile = MapTile(**tile_data)
                    session.merge(tile)
                    total += 1
                
                session.commit()
        
        elapsed = time.time() - start_time
        logger.info(
            f"[环境仓储] 批量更新 {total} 个地块，"
            f"耗时 {elapsed:.2f}s"
        )
        return total


# DEPRECATED: Module-level singleton
# Use container.environment_repository instead for proper isolation.
# This global instance will be removed in a future version.
environment_repository = EnvironmentRepository()
